{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61842d4c-2a40-475e-b685-0800365bbe4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4c1cbc3b031c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://17e348267994:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Optimizing Joins</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0100185910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Joins\")\n",
    "    .master(\"spark://17e348267994:7077\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70ab4bd-cb70-4de7-b97b-cb7313a5b6db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bb46c-8a0c-4f9b-bf78-4141d71f58f5",
   "metadata": {},
   "source": [
    "#### Join Big and Small table - SortMerge vs BroadCast Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aa89f9b-334a-4e68-a136-e22c272bc12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read EMP CSV data\n",
    "\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"/data/input/datasets/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4389aa4-73b7-4221-ab1c-6514dcf3b584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"/data/input/datasets/department_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "295111c4-987d-459c-b878-91ec2a9867aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_joined = emp.join(broadcast(dept), on=emp.department_id==dept.department_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a1e973-8a25-4b4c-90f7-79c74d18af31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56f16f9-61e1-4597-be0a-9db7440dd804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [department_id#7], [department_id#16], LeftOuter, BuildRight, false\n",
      ":- FileScan csv [first_name#0,last_name#1,job_title#2,dob#3,email#4,phone#5,salary#6,department_id#7] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/input/datasets/employee_records.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:string,email:string,phone:string,s...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#160]\n",
      "   +- *(1) Filter isnotnull(department_id#16)\n",
      "      +- FileScan csv [department_id#16,department_name#17,description#18,city#19,state#20,country#21] Batched: false, DataFilters: [isnotnull(department_id#16)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/input/datasets/department_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb129b-c3dc-4d22-85f1-4cd2881ea055",
   "metadata": {},
   "source": [
    "#### Join Big and Big table - SortMerge without Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d30c68-f7a9-4704-bcd9-6989cdd4298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales data\n",
    "\n",
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"/data/input/datasets/new_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e1278da-6a48-4035-ad0d-50aa8ce49fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read City data\n",
    "\n",
    "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"/data/input/datasets/cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ef56cb-f60f-4361-bc6e-a0ed9b5853e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join Data\n",
    "\n",
    "df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c7abf4-9174-49f6-91fa-a57216697bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169eb4f-5947-4f8b-8fa3-abce950055a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain Plan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3fdded-a234-4b83-b08f-06860ca75522",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Write Sales and City data in Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb3b217-40aa-4ccd-81fd-f4a5ad7be5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write Sales data in Buckets\n",
    "\n",
    "sales.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/input/datasets/sales_bucket.csv\").saveAsTable(\"sales_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc98941-e3f9-4b3d-8e6b-beac05bc658c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write City data in Buckets\n",
    "\n",
    "city.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/input/datasets/city_bucket.csv\").saveAsTable(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d59887b0-f929-414a-9237-6d8a4d7f4baa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default| city_bucket|      false|\n",
      "|  default|sales_bucket|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check tables\n",
    "\n",
    "spark.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929dc89-fdea-400b-affd-7003faae063d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Join Sales and City data - SortMerge with Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65f8c248-8bf9-4baf-b0c6-4dc56eb6a16f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read Sales table\n",
    "\n",
    "sales_bucket = spark.read.table(\"sales_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cf9264a-ee16-4f3f-be3c-ca5caa591217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read City table\n",
    "\n",
    "city_bucket = spark.read.table(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cad6d91-c2cc-4873-977e-42c5fa1f1f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join datasets\n",
    "\n",
    "df_joined_bucket = sales_bucket.join(city_bucket, on=sales_bucket.city_id==city_bucket.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19acc427-c43d-4006-b93a-093e0046d074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write dataset\n",
    "\n",
    "df_joined_bucket.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80bdfc6f-2454-45ef-ad4c-cebeadc96967",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) SortMergeJoin [city_id#176], [city_id#183], LeftOuter\n",
      ":- *(1) Sort [city_id#176 ASC NULLS FIRST], false, 0\n",
      ":  +- FileScan csv default.sales_bucket[transacted_at#171,trx_id#172,retailer_id#173,description#174,amount#175,city_id#176] Batched: false, Bucketed: true, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/input/datasets/sales_bucket.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit..., SelectedBucketsCount: 4 out of 4\n",
      "+- *(2) Sort [city_id#183 ASC NULLS FIRST], false, 0\n",
      "   +- *(2) Filter isnotnull(city_id#183)\n",
      "      +- FileScan csv default.city_bucket[city_id#183,city#184,state#185,state_abv#186,country#187] Batched: false, Bucketed: true, DataFilters: [isnotnull(city_id#183)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/input/datasets/city_bucket.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined_bucket.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ba247-8006-481d-98c1-537359cfe8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View how tasks are reading Bucket data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec46385-a66d-4fb3-bbb1-a42d0eefd65e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Points to note"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5c608d0-8fcb-4b61-b146-5fc0e1a4f4ad",
   "metadata": {},
   "source": [
    "1. Joining Column different than Bucket Column, Same Bucket Size - Shuffle on Both table\n",
    "2. Joining Column Same, One table in Bucket - Shuffle on non Bucket table\n",
    "3. Joining Column Same, Different Bucket Size - Shuffle on Smaller Bucket Side\n",
    "4. Joining Column Same, Same Bucket Size - No Shuffle (Faster Join)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93b1676c-c00d-4a0e-a5cb-7fbf16f110c9",
   "metadata": {},
   "source": [
    "1. So its very importatant to choose correct Bucket column and Bucket Size\n",
    "2. Decide effectively on number of Buckets, as too mant buckets with not enough data can lead to Small file issue.\n",
    "3. Datasets are Small - you can prefer Shuffle Hash Join"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
